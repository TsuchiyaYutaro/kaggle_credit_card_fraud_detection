{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import gc, datetime\n",
    "\n",
    "import optuna\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corret_card_id(x): \n",
    "    x=x.replace('.0','')\n",
    "    x=x.replace('-999','nan')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_indexes(df):\n",
    "    cards_cols= ['card1', 'card2', 'card3', 'card5']\n",
    "    for card in cards_cols: \n",
    "        if '1' in card: \n",
    "            df['card_id']= df[card].map(str)\n",
    "        else : \n",
    "            df['card_id']+= ' '+df[card].map(str)\n",
    "    df['card_id']=df['card_id'].apply(corret_card_id)\n",
    "    \n",
    "    df['uid'] = df['card1'].astype(str)+'_'+df['card2'].astype(str)+'_'+df['card3'].astype(str)+'_'+df['card4'].astype(str)\n",
    "\n",
    "    df['uid2'] = df['uid'].astype(str)+'_'+df['addr1'].astype(str)+'_'+df['addr2'].astype(str)\n",
    "\n",
    "    df['uid3'] = df['uid2'].astype(str)+'_'+df['addr1'].astype(str)+'_'+df['addr2'].astype(str)\n",
    "\n",
    "    df['uid4'] = df['card1'].astype(str)+'_'+df['card2'].astype(str)\n",
    "\n",
    "    df['uid5'] = df['uid'].astype(str)+'_'+df['card3'].astype(str)+'_'+df['card5'].astype(str)\n",
    "\n",
    "    df['uid6'] = df['uid2'].astype(str)+'_'+df['addr1'].astype(str)+'_'+df['addr2'].astype(str)\n",
    "\n",
    "    df['uid7'] = df['uid3'].astype(str)+'_'+df['P_emaildomain'].astype(str)\n",
    "\n",
    "    df['uid8'] = df['uid3'].astype(str)+'_'+df['R_emaildomain'].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_time(df):\n",
    "    START_DATE = '2017-12-01'\n",
    "    startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "    df['TransactionDT'] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "    df['year'] = df['TransactionDT'].dt.year\n",
    "    df['month'] = df['TransactionDT'].dt.month\n",
    "    df['dow'] = df['TransactionDT'].dt.dayofweek\n",
    "    df['hour'] = df['TransactionDT'].dt.hour\n",
    "    df['day'] = df['TransactionDT'].dt.day\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 端末情報\n",
    "def id_split(dataframe):\n",
    "    dataframe['device_name'] = dataframe['DeviceInfo'].str.split('/', expand=True)[0]\n",
    "    dataframe['device_version'] = dataframe['DeviceInfo'].str.split('/', expand=True)[1]\n",
    "\n",
    "    dataframe['OS_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[0]\n",
    "    dataframe['version_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[1]\n",
    "\n",
    "    dataframe['browser_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[0]\n",
    "    dataframe['version_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[1]\n",
    "\n",
    "    dataframe['screen_width'] = dataframe['id_33'].str.split('x', expand=True)[0]\n",
    "    dataframe['screen_height'] = dataframe['id_33'].str.split('x', expand=True)[1]\n",
    "\n",
    "    dataframe['id_34'] = dataframe['id_34'].str.split(':', expand=True)[1]\n",
    "    dataframe['id_23'] = dataframe['id_23'].str.split(':', expand=True)[1]\n",
    "\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n",
    "    dataframe.loc[dataframe['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n",
    "\n",
    "    dataframe.loc[dataframe.device_name.isin(dataframe.device_name.value_counts()[dataframe.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n",
    "    dataframe['had_id'] = 1\n",
    "    gc.collect()\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ブラウザ情報\n",
    "def setbrowser(df):\n",
    "    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n",
    "    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メールの情報補完\n",
    "def fix_emails(df):\n",
    "    df['P_emaildomain'] = df['P_emaildomain'].fillna('email_not_provided')\n",
    "    df['R_emaildomain'] = df['R_emaildomain'].fillna('email_not_provided')\n",
    "    df['email_match_not_nan'] = np.where((df['P_emaildomain']==df['R_emaildomain'])&(df['P_emaildomain']!='email_not_provided'),1,0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 542.35 Mb (69.4% reduction)\n",
      "Mem. usage decreased to 25.86 Mb (42.7% reduction)\n",
      "Mem. usage decreased to 472.59 Mb (68.9% reduction)\n",
      "Mem. usage decreased to 25.44 Mb (42.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_transaction = reduce_mem_usage(pd.read_csv('./ieee-fraud-detection/train_transaction.csv'))\n",
    "train_identity = reduce_mem_usage(pd.read_csv('./ieee-fraud-detection/train_identity.csv'))\n",
    "\n",
    "test_transaction = reduce_mem_usage(pd.read_csv('./ieee-fraud-detection/test_transaction.csv'))\n",
    "test_identity = reduce_mem_usage(pd.read_csv('./ieee-fraud-detection/test_identity.csv'))\n",
    "\n",
    "sample_submission = pd.read_csv('./ieee-fraud-detection/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06',\n",
      "       'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14',\n",
      "       'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22',\n",
      "       'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n",
      "       'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n",
      "       'DeviceType', 'DeviceInfo'],\n",
      "      dtype='object')\n",
      "Index(['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06',\n",
      "       'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14',\n",
      "       'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22',\n",
      "       'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n",
      "       'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n",
      "       'DeviceType', 'DeviceInfo'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_identity.columns)\n",
    "print(test_identity.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 端末情報の追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_identity = id_split(train_identity)\n",
    "test_identity = id_split(test_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デバイス情報の追加 : LB落ちるがCVが上がる\n",
    "train_identity['DeviceInfo'] = train_identity['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "test_identity['DeviceInfo'] = test_identity['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "\n",
    "train_identity['DeviceInfo_c'] = train_identity['DeviceInfo']\n",
    "test_identity['DeviceInfo_c'] = test_identity['DeviceInfo']\n",
    "\n",
    "device_match_dict = {\n",
    "    'sm':'sm-',\n",
    "    'sm':'samsung',\n",
    "    'huawei':'huawei',\n",
    "    'moto':'moto',\n",
    "    'rv':'rv:',\n",
    "    'trident':'trident',\n",
    "    'lg':'lg-',\n",
    "    'htc':'htc',\n",
    "    'blade':'blade',\n",
    "    'windows':'windows',\n",
    "    'lenovo':'lenovo',\n",
    "    'linux':'linux',\n",
    "    'f3':'f3',\n",
    "    'f5':'f5'\n",
    "}\n",
    "for dev_type_s, dev_type_o in device_match_dict.items():\n",
    "    train_identity['DeviceInfo_c'] = train_identity['DeviceInfo_c'].apply(lambda x: dev_type_s if dev_type_o in x else x)\n",
    "    test_identity['DeviceInfo_c'] = test_identity['DeviceInfo_c'].apply(lambda x: dev_type_s if dev_type_o in x else x)\n",
    "\n",
    "train_identity['DeviceInfo_c'] = train_identity['DeviceInfo_c'].apply(lambda x: 'other_d_type' if x not in device_match_dict else x)\n",
    "test_identity['DeviceInfo_c'] = test_identity['DeviceInfo_c'].apply(lambda x: 'other_d_type' if x not in device_match_dict else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デバイス情報2 : LB落ちるがCVが上がる\n",
    "train_identity['id_30'] = train_identity['id_30'].fillna('unknown_device').str.lower()\n",
    "test_identity['id_30'] = test_identity['id_30'].fillna('unknown_device').str.lower()\n",
    "\n",
    "train_identity['id_30_c'] = train_identity['id_30']\n",
    "test_identity['id_30_c'] = test_identity['id_30']\n",
    "\n",
    "device_match_dict = {\n",
    "    'ios':'ios',\n",
    "    'windows':'windows',\n",
    "    'mac':'mac',\n",
    "    'android':'android',\n",
    "    'linux':'linux'\n",
    "}\n",
    "for dev_type_s, dev_type_o in device_match_dict.items():\n",
    "    train_identity['id_30_c'] = train_identity['id_30_c'].apply(lambda x: dev_type_s if dev_type_o in x else x)\n",
    "    test_identity['id_30_c'] = test_identity['id_30_c'].apply(lambda x: dev_type_s if dev_type_o in x else x)\n",
    "    \n",
    "train_identity['id_30_v'] = train_identity['id_30'].apply(lambda x: ''.join([i for i in x if i.isdigit()]))\n",
    "test_identity['id_30_v'] = test_identity['id_30'].apply(lambda x: ''.join([i for i in x if i.isdigit()]))\n",
    "        \n",
    "train_identity['id_30_v'] = np.where(train_identity['id_30_v']!='', train_identity['id_30_v'], 0).astype(int)\n",
    "test_identity['id_30_v'] = np.where(test_identity['id_30_v']!='', test_identity['id_30_v'], 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ブラウザ情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LB落ちるがCVが上がる\n",
    "train_identity['id_31'] = train_identity['id_31'].fillna('unknown_br').str.lower()\n",
    "test_identity['id_31']  = test_identity['id_31'].fillna('unknown_br').str.lower()\n",
    "\n",
    "train_identity['id_31'] = train_identity['id_31'].apply(lambda x: x.replace('webview','webvw'))\n",
    "test_identity['id_31']  = test_identity['id_31'].apply(lambda x: x.replace('webview','webvw'))\n",
    "\n",
    "train_identity['id_31'] = train_identity['id_31'].apply(lambda x: x.replace('for',' '))\n",
    "test_identity['id_31']  = test_identity['id_31'].apply(lambda x: x.replace('for',' '))\n",
    "\n",
    "browser_list = set(list(train_identity['id_31'].unique()) + list(test_identity['id_31'].unique()))\n",
    "browser_list2 = []\n",
    "for item in browser_list:\n",
    "    browser_list2 += item.split(' ')\n",
    "browser_list2 = list(set(browser_list2))\n",
    "\n",
    "browser_list3 = []\n",
    "for item in browser_list2:\n",
    "    browser_list3 += item.split('/')\n",
    "browser_list3 = list(set(browser_list3))\n",
    "\n",
    "#for item in browser_list3:\n",
    "#    train_identity['id_31_e_'+item] = np.where(train_identity['id_31'].str.contains(item),1,0).astype(np.int8)\n",
    "#    test_identity['id_31_e_'+item] = np.where(test_identity['id_31'].str.contains(item),1,0).astype(np.int8)\n",
    "#    if train_identity['id_31_e_'+item].sum()<100:\n",
    "#        del train_identity['id_31_e_'+item], test_identity['id_31_e_'+item]\n",
    "        \n",
    "train_identity['id_31_v'] = train_identity['id_31'].apply(lambda x: ''.join([i for i in x if i.isdigit()]))\n",
    "test_identity['id_31_v'] = test_identity['id_31'].apply(lambda x: ''.join([i for i in x if i.isdigit()]))\n",
    "\n",
    "train_identity['id_31_v'] = np.where(train_identity['id_31_v']!='', train_identity['id_31_v'], 0).astype(int)\n",
    "test_identity['id_31_v'] = np.where(test_identity['id_31_v']!='', test_identity['id_31_v'], 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 欠損値の数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction['nulls_transaction'] = train_transaction.isna().sum(axis=1)\n",
    "train_transaction['nulls_cards'] = train_transaction[[\n",
    "    'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "    'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain',\n",
    "]].isna().sum(axis=1)\n",
    "train_transaction['nulls_C'] = train_transaction[[\n",
    "    'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9',\n",
    "    'C10', 'C11', 'C12', 'C13', 'C14',\n",
    "]].isna().sum(axis=1)\n",
    "train_transaction['nulls_D'] = train_transaction[[\n",
    "    'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9',\n",
    "    'D10', 'D11', 'D12', 'D13', 'D14', 'D15',\n",
    "]].isna().sum(axis=1)\n",
    "train_transaction['nulls_M'] = train_transaction[[\n",
    "    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n",
    "]].isna().sum(axis=1)\n",
    "train_transaction['nulls_V'] = train_transaction.iloc[:, 54:393].isna().sum(axis=1)\n",
    "\n",
    "train_identity['nulls_identity'] = train_identity.isna().sum(axis=1)\n",
    "train_identity['nulls_device'] = train_identity[[\n",
    "    'DeviceType', 'DeviceInfo'\n",
    "]].isna().sum(axis=1)\n",
    "train_identity['nulls_id'] = train_identity.iloc[:, 0:38].isna().sum(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "test_transaction['nulls_transaction'] = test_transaction.isna().sum(axis=1)\n",
    "test_transaction['nulls_cards'] = test_transaction[[\n",
    "    'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "    'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain',\n",
    "]].isna().sum(axis=1)\n",
    "test_transaction['nulls_C'] = test_transaction[[\n",
    "    'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9',\n",
    "    'C10', 'C11', 'C12', 'C13', 'C14',\n",
    "]].isna().sum(axis=1)\n",
    "test_transaction['nulls_D'] = test_transaction[[\n",
    "    'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9',\n",
    "    'D10', 'D11', 'D12', 'D13', 'D14', 'D15',\n",
    "]].isna().sum(axis=1)\n",
    "test_transaction['nulls_M'] = test_transaction[[\n",
    "    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n",
    "]].isna().sum(axis=1)\n",
    "test_transaction['nulls_V'] = test_transaction.iloc[:, 54:393].isna().sum(axis=1)\n",
    "\n",
    "test_identity['nulls_identity'] = test_identity.isna().sum(axis=1)\n",
    "test_identity['nulls_device'] = test_identity[[\n",
    "    'DeviceType', 'DeviceInfo'\n",
    "]].isna().sum(axis=1)\n",
    "test_identity['nulls_id'] = test_identity.iloc[:, 0:38].isna().sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# マージ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['nulls'] = train.isna().sum(axis=1)\n",
    "test['nulls'] = test.isna().sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cardID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = define_indexes(train)\n",
    "test = define_indexes(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# card1 のノイズの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBは上がるがCVはめっちゃ落ちる ダメそう\n",
    "#i_cols = ['card1']\n",
    "\n",
    "#for col in i_cols: \n",
    "#    valid_card = pd.concat([train[[col]], test[[col]]])\n",
    "#    valid_card = valid_card[col].value_counts()\n",
    "#    valid_card = valid_card[valid_card>2]\n",
    "#    valid_card = list(valid_card.index)\n",
    "\n",
    "#    train[col] = np.where(train[col].isin(test[col]), train[col], np.nan)\n",
    "#    test[col]  = np.where(test[col].isin(train[col]), test[col], np.nan)\n",
    "\n",
    "#    train[col] = np.where(train[col].isin(valid_card), train[col], np.nan)\n",
    "#    test[col]  = np.where(test[col].isin(valid_card), test[col], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProductCD と M4 の平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LB落ちるがCVが上がる\n",
    "for col in ['ProductCD','M4']:\n",
    "    temp_dict = train.groupby([col])['isFraud'].agg(['mean']).reset_index().rename(columns={'mean': col+'_target_mean'})\n",
    "    temp_dict.index = temp_dict[col].values\n",
    "    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n",
    "\n",
    "    train[col+'_target_mean'] = train[col].map(temp_dict)\n",
    "    test[col+'_target_mean']  = test[col].map(temp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 銀行情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "train['bank_type'] = train['card3'].astype(str)+'_'+train['card5'].astype(str)\n",
    "test['bank_type']  = test['card3'].astype(str)+'_'+test['card5'].astype(str)\n",
    "\n",
    "# ダメそう\n",
    "train['address_match'] = train['bank_type'].astype(str)+'_'+train['addr2'].astype(str)\n",
    "test['address_match']  = test['bank_type'].astype(str)+'_'+test['addr2'].astype(str)\n",
    "\n",
    "for col in ['address_match','bank_type']:\n",
    "    temp = pd.concat([train[[col]], test[[col]]])\n",
    "    temp[col] = np.where(temp[col].str.contains('nan'), np.nan, temp[col])\n",
    "    temp = temp.dropna()\n",
    "    fq_encode = temp[col].value_counts().to_dict()   \n",
    "    train[col] = train[col].map(fq_encode)\n",
    "    test[col]  = test[col].map(fq_encode)\n",
    "\n",
    "train['address_match'] = train['address_match']/train['bank_type'] \n",
    "test['address_match']  = test['address_match']/test['bank_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交差項"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n",
    "                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n",
    "    f1, f2 = feature.split('__')\n",
    "    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n",
    "    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n",
    "    train[feature] = le.transform(list(train[feature].astype(str).values))\n",
    "    test[feature] = le.transform(list(test[feature].astype(str).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['id_34', 'id_36']:\n",
    "    train[feature + '_count_full'] = train[feature].map(\n",
    "        pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n",
    "    test[feature + '_count_full'] = test[feature].map(\n",
    "        pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['id_01', 'id_31', 'id_33', 'id_35', 'id_36']:\n",
    "    train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n",
    "    test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "train['card1_card2'] = train['card1'].astype(str) + '_' + train['card2'].astype(str)\n",
    "train['addr1_dist1'] = train['addr1'].astype(str) + '_' + train['dist1'].astype(str)\n",
    "train['card1_addr2'] = train['card1'].astype(str) + '_' + train['addr2'].astype(str)\n",
    "train['card2_addr1'] = train['card2'].astype(str) + '_' + train['addr1'].astype(str)\n",
    "train['card2_addr2'] = train['card2'].astype(str) + '_' + train['addr2'].astype(str)\n",
    "train['card4_addr1'] = train['card4'].astype(str) + '_' + train['addr1'].astype(str)\n",
    "train['card4_addr2'] = train['card4'].astype(str) + '_' + train['addr2'].astype(str)\n",
    "train['P_emaildomain_addr1'] = train['P_emaildomain'].astype(str) + '_' + train['addr1'].astype(str)\n",
    "train['id01_addr1'] = train['id_01'].astype(str) + '_' + train['addr1'].astype(str)\n",
    "\n",
    "test['card1_card2'] = test['card1'].astype(str) + '_' + test['card2'].astype(str)\n",
    "test['addr1_dist1'] = test['addr1'].astype(str) + '_' + test['dist1'].astype(str)\n",
    "test['card1_addr2'] = test['card1'].astype(str) + '_' + test['addr2'].astype(str)\n",
    "test['card2_addr1'] = test['card2'].astype(str) + '_' + test['addr1'].astype(str)\n",
    "test['card2_addr2'] = test['card2'].astype(str) + '_' + test['addr2'].astype(str)\n",
    "test['card4_addr1'] = test['card4'].astype(str) + '_' + test['addr1'].astype(str)\n",
    "test['card4_addr2'] = test['card4'].astype(str) + '_' + test['addr2'].astype(str)\n",
    "test['P_emaildomain_addr1'] = test['P_emaildomain'].astype(str) + '_' + test['addr1'].astype(str)\n",
    "test['id01_addr1'] = test['id_01'].astype(str) + '_' + test['addr1'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ブラウザ情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(train.shape[0])\n",
    "train[\"lastest_browser\"] = a\n",
    "train=setbrowser(train)\n",
    "\n",
    "a = np.zeros(test.shape[0])\n",
    "test[\"lastest_browser\"] = a\n",
    "test=setbrowser(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カードごとの枚数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['card1_count_full'] = train['card1'].map(pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False))\n",
    "train['card2_count_full'] = train['card2'].map(pd.concat([train['card2'], test['card2']], ignore_index=True).value_counts(dropna=False))\n",
    "train['card3_count_full'] = train['card3'].map(pd.concat([train['card3'], test['card3']], ignore_index=True).value_counts(dropna=False))\n",
    "train['card4_count_full'] = train['card4'].map(pd.concat([train['card4'], test['card4']], ignore_index=True).value_counts(dropna=False))\n",
    "train['card5_count_full'] = train['card5'].map(pd.concat([train['card5'], test['card5']], ignore_index=True).value_counts(dropna=False))\n",
    "train['card6_count_full'] = train['card6'].map(pd.concat([train['card6'], test['card6']], ignore_index=True).value_counts(dropna=False))\n",
    "train['addr1_count_full'] = train['addr1'].map(pd.concat([train['addr1'], test['addr1']], ignore_index=True).value_counts(dropna=False))\n",
    "train['addr2_count_full'] = train['addr2'].map(pd.concat([train['addr2'], test['addr2']], ignore_index=True).value_counts(dropna=False))\n",
    "train['P_emaildomain_count_full'] = train['P_emaildomain'].map(pd.concat([train['P_emaildomain'], test['P_emaildomain']], ignore_index=True).value_counts(dropna=False))\n",
    "train['R_emaildomain_count_full'] = train['R_emaildomain'].map(pd.concat([train['R_emaildomain'], test['R_emaildomain']], ignore_index=True).value_counts(dropna=False))\n",
    "train['uid_count_full'] = train['uid'].map(pd.concat([train['uid'], test['uid']], ignore_index=True).value_counts(dropna=False))\n",
    "train['uid2_count_full'] = train['uid2'].map(pd.concat([train['uid2'], test['uid2']], ignore_index=True).value_counts(dropna=False))\n",
    "train['uid3_count_full'] = train['uid3'].map(pd.concat([train['uid3'], test['uid3']], ignore_index=True).value_counts(dropna=False))\n",
    "train['uid4_count_full'] = train['uid4'].map(pd.concat([train['uid4'], test['uid4']], ignore_index=True).value_counts(dropna=False))\n",
    "train['uid5_count_full'] = train['uid5'].map(pd.concat([train['uid5'], test['uid5']], ignore_index=True).value_counts(dropna=False))\n",
    "train['uid6_count_full'] = train['uid6'].map(pd.concat([train['uid6'], test['uid6']], ignore_index=True).value_counts(dropna=False))\n",
    "train['uid7_count_full'] = train['uid7'].map(pd.concat([train['uid7'], test['uid7']], ignore_index=True).value_counts(dropna=False))\n",
    "train['uid8_count_full'] = train['uid8'].map(pd.concat([train['uid8'], test['uid8']], ignore_index=True).value_counts(dropna=False))\n",
    "train['card_id_count_full'] = train['card_id'].map(pd.concat([train['card_id'], test['card_id']], ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "test['card1_count_full'] = test['card1'].map(pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False))\n",
    "test['card2_count_full'] = test['card2'].map(pd.concat([train['card2'], test['card2']], ignore_index=True).value_counts(dropna=False))\n",
    "test['card3_count_full'] = test['card3'].map(pd.concat([train['card3'], test['card3']], ignore_index=True).value_counts(dropna=False))\n",
    "test['card4_count_full'] = test['card4'].map(pd.concat([train['card4'], test['card4']], ignore_index=True).value_counts(dropna=False))\n",
    "test['card5_count_full'] = test['card5'].map(pd.concat([train['card5'], test['card5']], ignore_index=True).value_counts(dropna=False))\n",
    "test['card6_count_full'] = test['card6'].map(pd.concat([train['card6'], test['card6']], ignore_index=True).value_counts(dropna=False))\n",
    "test['addr1_count_full'] = test['addr1'].map(pd.concat([train['addr1'], test['addr1']], ignore_index=True).value_counts(dropna=False))\n",
    "test['addr2_count_full'] = test['addr2'].map(pd.concat([train['addr2'], test['addr2']], ignore_index=True).value_counts(dropna=False))\n",
    "test['P_emaildomain_count_full'] = test['P_emaildomain'].map(pd.concat([train['P_emaildomain'], test['P_emaildomain']], ignore_index=True).value_counts(dropna=False))\n",
    "test['R_emaildomain_count_full'] = test['R_emaildomain'].map(pd.concat([train['R_emaildomain'], test['R_emaildomain']], ignore_index=True).value_counts(dropna=False))\n",
    "test['uid_count_full'] = test['uid'].map(pd.concat([train['uid'], test['uid']], ignore_index=True).value_counts(dropna=False))\n",
    "test['uid2_count_full'] = test['uid2'].map(pd.concat([train['uid2'], test['uid2']], ignore_index=True).value_counts(dropna=False))\n",
    "test['uid3_count_full'] = test['uid3'].map(pd.concat([train['uid3'], test['uid3']], ignore_index=True).value_counts(dropna=False))\n",
    "test['uid4_count_full'] = test['uid4'].map(pd.concat([train['uid4'], test['uid4']], ignore_index=True).value_counts(dropna=False))\n",
    "test['uid5_count_full'] = test['uid5'].map(pd.concat([train['uid5'], test['uid5']], ignore_index=True).value_counts(dropna=False))\n",
    "test['uid6_count_full'] = test['uid6'].map(pd.concat([train['uid6'], test['uid6']], ignore_index=True).value_counts(dropna=False))\n",
    "test['uid7_count_full'] = test['uid7'].map(pd.concat([train['uid7'], test['uid7']], ignore_index=True).value_counts(dropna=False))\n",
    "test['uid8_count_full'] = test['uid8'].map(pd.concat([train['uid8'], test['uid8']], ignore_index=True).value_counts(dropna=False))\n",
    "test['card_id_count_full'] = test['card_id'].map(pd.concat([train['card_id'], test['card_id']], ignore_index=True).value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 金額"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# めっちゃ上がる\n",
    "valid_card = train['TransactionAmt'].value_counts()\n",
    "valid_card = valid_card[valid_card>10]\n",
    "valid_card = list(valid_card.index)\n",
    "\n",
    "train['TransactionAmt_check'] = np.where(train['TransactionAmt'].isin(test['TransactionAmt']), 1, 0)\n",
    "test['TransactionAmt_check']  = np.where(test['TransactionAmt'].isin(train['TransactionAmt']), 1, 0)\n",
    "\n",
    "i_cols = ['card1','card2','card3','card4','card5','card6','uid','uid2','uid3','uid4','uid5','uid6','uid7','uid8']\n",
    "\n",
    "for col in i_cols:\n",
    "    for agg_type in ['mean', 'std']:\n",
    "        new_col_name = col+'_TransactionAmt_'+agg_type\n",
    "        temp = pd.concat([train[[col, 'TransactionAmt']], test[[col,'TransactionAmt']]])\n",
    "        temp = temp.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(columns={agg_type: new_col_name})\n",
    "        \n",
    "        temp.index = list(temp[col])\n",
    "        temp = temp[new_col_name].to_dict()   \n",
    "    \n",
    "        train[new_col_name] = train[col].map(temp)\n",
    "        test[new_col_name]  = test[col].map(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "train['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "# 以下、効かなそう\n",
    "train['TransactionAmt_to_mean_card_id'] = train['TransactionAmt'] / train.groupby(['card_id'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_std_card_id'] = train['TransactionAmt_to_mean_card_id'] / train.groupby(['card_id'])['TransactionAmt'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "test['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "# 以下、効かなそう\n",
    "test['TransactionAmt_to_mean_card_id'] = test['TransactionAmt'] / test.groupby(['card_id'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_std_card_id'] = test['TransactionAmt_to_mean_card_id'] / test.groupby(['card_id'])['TransactionAmt'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "train['TransactionAmt_to_mean_card5'] = train['TransactionAmt'] / train.groupby(['card5'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_mean_addr1'] = train['TransactionAmt'] / train.groupby(['addr1'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_mean_id31'] = train['TransactionAmt'] / train.groupby(['id_31'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_mean_devicename'] = train['TransactionAmt'] / train.groupby(['device_name'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_std_card5'] = train['TransactionAmt'] / train.groupby(['card5'])['TransactionAmt'].transform('std')\n",
    "train['TransactionAmt_to_std_addr1'] = train['TransactionAmt'] / train.groupby(['addr1'])['TransactionAmt'].transform('std')\n",
    "train['TransactionAmt_to_std_id31'] = train['TransactionAmt'] / train.groupby(['id_31'])['TransactionAmt'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['TransactionAmt_to_mean_card5'] = test['TransactionAmt'] / test.groupby(['card5'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_mean_addr1'] = test['TransactionAmt'] / test.groupby(['addr1'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_mean_id31'] = test['TransactionAmt'] / test.groupby(['id_31'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_mean_devicename'] = test['TransactionAmt'] / test.groupby(['device_name'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_std_card5'] = test['TransactionAmt'] / test.groupby(['card5'])['TransactionAmt'].transform('std')\n",
    "test['TransactionAmt_to_std_addr1'] = test['TransactionAmt'] / test.groupby(['addr1'])['TransactionAmt'].transform('std')\n",
    "test['TransactionAmt_to_std_id31'] = test['TransactionAmt'] / test.groupby(['id_31'])['TransactionAmt'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "train['Trans_min_mean'] = train['TransactionAmt'] - train['TransactionAmt'].mean()\n",
    "train['Trans_min_std'] = train['Trans_min_mean'] / train['TransactionAmt'].std()\n",
    "\n",
    "test['Trans_min_mean'] = test['TransactionAmt'] - test['TransactionAmt'].mean()\n",
    "test['Trans_min_std'] = test['Trans_min_mean'] / test['TransactionAmt'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 金額の小数点\n",
    "train['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\n",
    "test['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TransactionAmt'] = np.log1p(train['TransactionAmt'])\n",
    "test['TransactionAmt'] = np.log1p(test['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ユーザ情報×日付情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n",
    "    df['DT_M'] = (df['DT'].dt.year-2017)*12 + df['DT'].dt.month\n",
    "    df['DT_W'] = (df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear\n",
    "    df['DT_D'] = (df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear\n",
    "    \n",
    "    # D9 column\n",
    "    df['D9'] = np.where(df['D9'].isna(),0,1)\n",
    "\n",
    "periods = ['DT_M','DT_W','DT_D']\n",
    "i_cols = ['card1','card2','card3','card4','card5','card6','uid','uid2','uid3','uid4','uid5','uid6','uid7','uid8']\n",
    "for period in periods:\n",
    "    for col in i_cols:\n",
    "        new_column = col + '_' + period\n",
    "            \n",
    "        temp = pd.concat([train[[col,period]], test[[col,period]]])\n",
    "        temp[new_column] = temp[col].astype(str) + '_' + (temp[period]).astype(str)\n",
    "        fq_encode = temp[new_column].value_counts().to_dict()\n",
    "            \n",
    "        train[new_column] = (train[col].astype(str) + '_' + train[period].astype(str)).map(fq_encode)\n",
    "        test[new_column]  = (test[col].astype(str) + '_' + test[period].astype(str)).map(fq_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\n",
    "us_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['is_december'] = df['DT'].dt.month\n",
    "    df['is_december'] = (df['is_december']==12).astype(np.int8)\n",
    "\n",
    "    df['is_holiday'] = (df['DT'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "i_cols = ['D'+str(i) for i in range(1,16)]\n",
    "uids = ['card1','card2','card3','card4','card5','card6','uid','uid2','uid3','uid4','uid5','uid6','uid7','uid8']\n",
    "\n",
    "for df in [train, test]:\n",
    "    for col in i_cols:\n",
    "        df[col] = df[col].clip(0) \n",
    "    df['D9_not_na'] = np.where(df['D9'].isna(),0,1)\n",
    "    df['D8_not_same_day'] = np.where(df['D8']>=1,1,0)\n",
    "    df['D8_D9_decimal_dist'] = df['D8'].fillna(0)-df['D8'].fillna(0).astype(int)\n",
    "    df['D8_D9_decimal_dist'] = ((df['D8_D9_decimal_dist']-df['D9'])**2)**0.5\n",
    "    df['D8'] = df['D8'].fillna(-1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通信の情報 id_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\n",
    "train['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\n",
    "train['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\n",
    "train['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n",
    "\n",
    "# 以下、効かなそう\n",
    "train['id_02_to_mean_card_id'] = train['id_02'] / train.groupby(['card_id'])['id_02'].transform('mean')\n",
    "train['id_02_to_std_card_id'] = train['id_02_to_mean_card_id'] / train.groupby(['card_id'])['id_02'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\n",
    "test['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\n",
    "test['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\n",
    "test['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n",
    "\n",
    "# 以下、効かなそう\n",
    "test['id_02_to_mean_card_id'] = test['id_02'] / test.groupby(['card_id'])['id_02'].transform('mean')\n",
    "test['id_02_to_std_card_id'] = test['id_02_to_mean_card_id'] / test.groupby(['card_id'])['id_02'].transform('std')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 頻度情報 D15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\n",
    "train['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\n",
    "train['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\n",
    "train['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n",
    "train['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\n",
    "train['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\n",
    "# TODO\n",
    "train['D15_to_mean_addr2'] = train['D15'] / train.groupby(['addr2'])['D15'].transform('mean')\n",
    "train['D15_to_std_addr2'] = train['D15'] / train.groupby(['addr2'])['D15'].transform('std')\n",
    "\n",
    "# 以下、効かなそう\n",
    "train['D15_to_mean_card_id'] = train['D15'] / train.groupby(['card_id'])['D15'].transform('mean')\n",
    "train['D15_to_std_card_id'] = train['D15_to_mean_card_id'] / train.groupby(['card_id'])['D15'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\n",
    "test['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\n",
    "test['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\n",
    "test['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n",
    "test['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\n",
    "test['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\n",
    "# TODO\n",
    "test['D15_to_mean_addr2'] = test['D15'] / test.groupby(['addr2'])['D15'].transform('mean')\n",
    "test['D15_to_std_addr2'] = test['D15'] / test.groupby(['addr2'])['D15'].transform('std')\n",
    "\n",
    "# 以下、効かなそう\n",
    "test['D15_to_mean_card_id'] = test['D15'] / test.groupby(['card_id'])['D15'].transform('mean')\n",
    "test['D15_to_std_card_id'] = test['D15_to_mean_card_id'] / test.groupby(['card_id'])['D15'].transform('std')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 距離情報 : original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dist1_to_mean_card1'] = train['dist1'] / train.groupby(['card1'])['dist1'].transform('mean')\n",
    "train['dist1_to_mean_card4'] = train['dist1'] / train.groupby(['card4'])['dist1'].transform('mean')\n",
    "train['dist1_to_std_card1'] = train['dist1'] / train.groupby(['card1'])['dist1'].transform('std')\n",
    "train['dist1_to_std_card4'] = train['dist1'] / train.groupby(['card4'])['dist1'].transform('std')\n",
    "# ダメ\n",
    "train['dist1_to_mean_addr1'] = train['dist1'] / train.groupby(['addr1'])['dist1'].transform('mean')\n",
    "train['dist1_to_std_addr1'] = train['dist1'] / train.groupby(['addr1'])['dist1'].transform('std')\n",
    "\n",
    "# 以下、効かなそう\n",
    "train['dist1_to_mean_card_id'] = train['dist1'] / train.groupby(['card_id'])['dist1'].transform('mean')\n",
    "train['dist1_to_std_card_id'] = train['dist1_to_mean_card_id'] / train.groupby(['card_id'])['dist1'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['dist1_to_mean_card1'] = test['dist1'] / test.groupby(['card1'])['dist1'].transform('mean')\n",
    "test['dist1_to_mean_card4'] = test['dist1'] / test.groupby(['card4'])['dist1'].transform('mean')\n",
    "test['dist1_to_std_card1'] = test['dist1'] / test.groupby(['card1'])['dist1'].transform('std')\n",
    "test['dist1_to_std_card4'] = test['dist1'] / test.groupby(['card4'])['dist1'].transform('std')\n",
    "# ダメ\n",
    "test['dist1_to_mean_addr1'] = test['dist1'] / test.groupby(['addr1'])['dist1'].transform('mean')\n",
    "test['dist1_to_std_addr1'] = test['dist1'] / test.groupby(['addr1'])['dist1'].transform('std')\n",
    "\n",
    "# 以下、効かなそう\n",
    "test['dist1_to_mean_card_id'] = test['dist1'] / test.groupby(['card_id'])['dist1'].transform('mean')\n",
    "test['dist1_to_std_card_id'] = test['dist1_to_mean_card_id'] / test.groupby(['card_id'])['dist1'].transform('std')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 直近の情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#latest = 10\n",
    "#uids = ['card1','card2','card3','card4','card5','card6','uid','uid2','uid3','uid4','uid5','uid6','uid7','uid8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for uid in uids:\n",
    "#    train['TransactionAmt_rolling_mean_'+uid] = train.groupby([uid])['TransactionAmt'].apply(lambda x:x.rolling(latest, 1).mean())\n",
    "#    train['TransactionAmt_rolling_mean_per_'+uid] = train['TransactionAmt']/train['TransactionAmt_rolling_mean_'+uid]\n",
    "#    train['TransactionAmt_rolling_std_'+uid] = train.groupby([uid])['TransactionAmt'].apply(lambda x:x.rolling(latest, 1).std())\n",
    "#    train['TransactionAmt_rolling_std_per_'+uid] = train['TransactionAmt']/train['TransactionAmt_rolling_std_'+uid]\n",
    "#    train['dist1_rolling_mean_'+uid] = train.groupby([uid])['dist1'].apply(lambda x:x.rolling(latest, 1).mean())\n",
    "#    train['dist1_rolling_mean_per_'+uid] = train['dist1']/train['dist1_rolling_mean_'+uid]\n",
    "#    train['dist1_rolling_std_'+uid] = train.groupby([uid])['dist1'].apply(lambda x:x.rolling(latest, 1).std())\n",
    "#    train['dist1_rolling_std_per_'+uid] = train['dist1']/train['dist1_rolling_std_'+uid]\n",
    "    \n",
    "#    test['TransactionAmt_rolling_mean_'+uid] = test.groupby([uid])['TransactionAmt'].apply(lambda x:x.rolling(latest, 1).mean())\n",
    "#    test['TransactionAmt_rolling_mean_per_'+uid] = test['TransactionAmt']/test['TransactionAmt_rolling_mean_'+uid]\n",
    "#    test['TransactionAmt_rolling_std_'+uid] = test.groupby([uid])['TransactionAmt'].apply(lambda x:x.rolling(latest, 1).std())\n",
    "#    test['TransactionAmt_rolling_std_per_'+uid] = test['TransactionAmt']/test['TransactionAmt_rolling_std_'+uid]\n",
    "#    test['dist1_rolling_mean_'+uid] = test.groupby([uid])['dist1'].apply(lambda x:x.rolling(latest, 1).mean())\n",
    "#    test['dist1_rolling_mean_per_'+uid] = test['dist1']/test['dist1_rolling_mean_'+uid]\n",
    "#    test['dist1_rolling_std_'+uid] = test.groupby([uid])['dist1'].apply(lambda x:x.rolling(latest, 1).std())\n",
    "#    test['dist1_rolling_std_per_'+uid] = test['dist1']/test['dist1_rolling_std_'+uid]\n",
    "#    print(uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カードの情報の分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['first_value_card1'] = train.loc[~train['card1'].isnull(), 'card1'].astype(str).str[0:1].astype(float)\n",
    "train['two_value_card1'] = train.loc[~train['card1'].isnull(), 'card1'].astype(str).str[0:2].astype(float)\n",
    "train['card2'] = train['card2'].fillna(0)\n",
    "train['first_value_card2'] = train['card2'].astype(str).str[0:1].astype(float)\n",
    "train['two_value_card2'] = train['card2'].astype(str).str[0:2].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['first_value_card1'] = test.loc[~test['card1'].isnull(), 'card1'].astype(str).str[0:1].astype(float)\n",
    "test['two_value_card1'] = test.loc[~test['card1'].isnull(), 'card1'].astype(str).str[0:2].astype(float)\n",
    "test['card2'] = test['card2'].fillna(0)\n",
    "test['first_value_card2'] = test['card2'].astype(str).str[0:1].astype(float)\n",
    "test['two_value_card2'] = test['card2'].astype(str).str[0:2].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## email の情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum',\n",
    "          'scranton.edu': 'other', 'netzero.net': 'other',\n",
    "          'optonline.net': 'other', 'comcast.net': 'other', \n",
    "          'cfl.rr.com': 'other', 'sc.rr.com': 'other',\n",
    "          'suddenlink.net': 'other', 'windstream.net': 'other',\n",
    "          'gmx.de': 'other', 'earthlink.net': 'other', \n",
    "          'servicios-ta.com': 'other', 'bellsouth.net': 'other', \n",
    "          'web.de': 'other', 'mail.com': 'other',\n",
    "          'cableone.net': 'other', 'roadrunner.com': 'other', \n",
    "          'protonmail.com': 'other', 'anonymous.com': 'other',\n",
    "          'juno.com': 'other', 'ptd.net': 'other',\n",
    "          'netzero.com': 'other', 'cox.net': 'other', \n",
    "          'hotmail.co.uk': 'microsoft', \n",
    "          'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', \n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', \n",
    "          'live.com': 'microsoft', 'aim.com': 'aol',\n",
    "          'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n",
    "          'gmail.com': 'google', 'me.com': 'apple', \n",
    "          'hotmail.com': 'microsoft',  \n",
    "          'hotmail.fr': 'microsoft',\n",
    "          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', \n",
    "          'yahoo.de': 'yahoo', \n",
    "          'live.fr': 'microsoft', 'verizon.net': 'yahoo', \n",
    "          'msn.com': 'microsoft', 'q.com': 'centurylink',\n",
    "          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', \n",
    "           'rocketmail.com': 'yahoo', \n",
    "          'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', \n",
    "          'embarqmail.com': 'centurylink', \n",
    "          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo',\n",
    "          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft',\n",
    "           'aol.com': 'aol', 'icloud.com': 'apple'}\n",
    "\n",
    "us_emails = ['gmail', 'net', 'edu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    train[c + '_bin'] = train[c].map(emails)\n",
    "    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n",
    "    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    \n",
    "    test[c + '_bin'] = test[c].map(emails)\n",
    "    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P_R メールの関係性\n",
    "train['email_domain_comp'] = (train['P_emaildomain'].values == train['R_emaildomain'].values).astype(int)\n",
    "train['email_domain_suffix_bin'] = (train['P_emaildomain_bin'].values == train['R_emaildomain_bin'].values).astype(int)\n",
    "train['email_domain_suffix_comp'] = (train['P_emaildomain_suffix'].values == train['R_emaildomain_suffix'].values).astype(int)\n",
    "\n",
    "test['email_domain_comp'] = (test['P_emaildomain'].values == test['R_emaildomain'].values).astype(int)\n",
    "test['email_domain_suffix_bin'] = (test['P_emaildomain_bin'].values == test['R_emaildomain_bin'].values).astype(int)\n",
    "test['email_domain_suffix_comp'] = (test['P_emaildomain_suffix'].values == test['R_emaildomain_suffix'].values).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロトンメールか\n",
    "train['P_isprotonmail'] = 0\n",
    "train.loc[train['P_emaildomain']=='protonmail.com', 'P_isprotonmail'] = 1\n",
    "train['R_isprotonmail'] = 0\n",
    "train.loc[train['R_emaildomain']=='protonmail.com', 'R_isprotonmail'] = 1\n",
    "\n",
    "test['P_isprotonmail'] = 0\n",
    "test.loc[test['P_emaildomain']=='protonmail.com', 'P_isprotonmail'] = 1\n",
    "test['R_isprotonmail'] = 0\n",
    "test.loc[test['R_emaildomain']=='protonmail.com', 'R_isprotonmail'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メールの欠損情報 : LB上がるがCVが落ちる\n",
    "train['email_check_nan_all'] = np.where((train['P_emaildomain'].isna())&(train['R_emaildomain'].isna()),1,0)\n",
    "test['email_check_nan_all']  = np.where((test['P_emaildomain'].isna())&(test['R_emaildomain'].isna()),1,0)\n",
    "\n",
    "train['email_check_nan_any'] = np.where((train['P_emaildomain'].isna())|(train['R_emaildomain'].isna()),1,0)\n",
    "test['email_check_nan_any']  = np.where((test['P_emaildomain'].isna())|(test['R_emaildomain'].isna()),1,0)\n",
    "\n",
    "train = fix_emails(train)\n",
    "test = fix_emails(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D9 : 時間っぽいけど何これ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダメ\n",
    "train['local_hour'] = train['D9']*24\n",
    "test['local_hour']  = test['D9']*24\n",
    "\n",
    "train['local_hour'] = train['local_hour'] - (train['TransactionDT']/(60*60))%24\n",
    "test['local_hour']  = test['local_hour'] - (test['TransactionDT']/(60*60))%24\n",
    "\n",
    "train['local_hour_dist'] = train['local_hour']/train['dist2']\n",
    "test['local_hour_dist']  = test['local_hour']/test['dist2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M系 : アドレスとかの boolean データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダメ\n",
    "i_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n",
    "\n",
    "train['M_sum'] = train[i_cols].sum(axis=1).astype(np.int8)\n",
    "test['M_sum']  = test[i_cols].sum(axis=1).astype(np.int8)\n",
    "\n",
    "train['M_na'] = train[i_cols].isna().sum(axis=1).astype(np.int8)\n",
    "test['M_na']  = test[i_cols].isna().sum(axis=1).astype(np.int8)\n",
    "\n",
    "train['M_type'] = ''\n",
    "test['M_type']  = ''\n",
    "\n",
    "for col in i_cols:\n",
    "    train['M_type'] = '_'+train[col].astype(str)\n",
    "    test['M_type'] = '_'+test[col].astype(str)\n",
    "\n",
    "# TODO\n",
    "i_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['M_sum'] = df[i_cols].sum(axis=1).astype(np.int8)\n",
    "    df['M_na'] = df[i_cols].isna().sum(axis=1).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C系 : 支払いに関連づけられる情報 : TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_cols = ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']\n",
    "\n",
    "train['C_sum'] = 0\n",
    "test['C_sum']  = 0\n",
    "\n",
    "train['C_null'] = 0\n",
    "test['C_null']  = 0\n",
    "\n",
    "for col in i_cols:\n",
    "    train['C_sum'] += np.where(train[col]==1,1,0)\n",
    "    test['C_sum']  += np.where(test[col]==1,1,0)\n",
    "\n",
    "    train['C_null'] += np.where(train[col]==0,1,0)\n",
    "    test['C_null']  += np.where(test[col]==0,1,0)\n",
    "    \n",
    "    valid_values = train[col].value_counts()\n",
    "    valid_values = valid_values[valid_values>1000]\n",
    "    valid_values = list(valid_values.index)\n",
    "    \n",
    "    train[col+'_valid'] = np.where(train[col].isin(valid_values),1,0)\n",
    "    test[col+'_valid']  = np.where(test[col].isin(valid_values),1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カード情報、日付情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = define_time(train)\n",
    "test = define_time(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 頻度情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上がる\n",
    "i_cols = ['card1','card2','card3','card4','card5','card6',\n",
    "          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "          'D1','D2','D3','D4','D5','D6','D7','D8','D9',\n",
    "          'addr1','addr2',\n",
    "          'dist1','dist2',\n",
    "          'P_emaildomain', 'R_emaildomain',\n",
    "          'id_01','id_02','id_03','id_04','id_05','id_06','id_07','id_08','id_09','id_10',\n",
    "          'id_11','id_13','id_14','id_17','id_18','id_19','id_20','id_21','id_22','id_24',\n",
    "          'id_25','id_26','id_30','id_31','id_32','id_33',\n",
    "          'DeviceInfo','DeviceInfo_c','id_30_c','id_30_v','id_31_v',\n",
    "          'uid','uid2','uid3','uid4','uid5','uid6','uid7','uid8','card_id',\n",
    "          'year','month','dow','hour','day',\n",
    "          'DT_M','DT_W','DT_D',\n",
    "         ]\n",
    "\n",
    "for col in i_cols:\n",
    "    temp = pd.concat([train[[col]], test[[col]]])\n",
    "    fq_encode = temp[col].value_counts().to_dict()   \n",
    "    train[col+'_fq_enc'] = train[col].map(fq_encode)\n",
    "    test[col+'_fq_enc']  = test[col].map(fq_encode)\n",
    "    \n",
    "    \n",
    "#リーク\n",
    "#i_cols = ['card1','card2','card3','card4','card5','card6',\n",
    "#          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "#          'D1','D2','D3','D4','D5','D6','D7','D8','D9',\n",
    "#          'addr1','addr2',\n",
    "#          'dist1','dist2',\n",
    "#          'P_emaildomain', 'R_emaildomain',\n",
    "#          'id_01','id_02','id_03','id_04','id_05','id_06','id_07','id_08','id_09','id_10',\n",
    "#          'id_11','id_13','id_14','id_17','id_18','id_19','id_20','id_21','id_22','id_24',\n",
    "#          'id_25','id_26','id_30','id_31','id_32','id_33',\n",
    "#          'DeviceInfo','DeviceInfo_c','id_30_c','id_30_v','id_31_v',\n",
    "#          'uid','uid2','uid3','uid4','uid5','uid6','uid7','uid8','card_id',\n",
    "#          'year','month','dow','hour','day',\n",
    "#          'DT_M','DT_W','DT_D',\n",
    "#         ]\n",
    "\n",
    "#for col in i_cols:\n",
    "#    temp_dict = train.groupby([col])['isFraud'].agg(['mean']).reset_index().rename(columns={'mean': col+'_target_mean'})\n",
    "#    temp_dict.index = temp_dict[col].values\n",
    "#    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n",
    "\n",
    "#    train[col+'_target_mean'] = train[col].map(temp_dict)\n",
    "#    test[col+'_target_mean']  = test[col].map(temp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# エンコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# オブジェクトの変換 : LBは最高だがCVが低くなる\n",
    "#for col in list(X_train):\n",
    "#    if X_train[col].dtype=='O':\n",
    "#        X_train[col] = X_train[col].fillna('unseen_before_label')\n",
    "#        X_test[col]  = X_test[col].fillna('unseen_before_label')\n",
    "        \n",
    "#        X_train[col] = X_train[col].astype(str)\n",
    "#        X_test[col] = X_test[col].astype(str)\n",
    "        \n",
    "#        le = LabelEncoder()\n",
    "#        le.fit(list(X_train[col])+list(X_test[col]))\n",
    "#        X_train[col] = le.transform(X_train[col])\n",
    "#        X_test[col]  = le.transform(X_test[col])\n",
    "        \n",
    "#        X_train[col] = X_train[col].astype('category')\n",
    "#        X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "for f in train.select_dtypes(include='category').columns.tolist() + train.select_dtypes(include='object').columns.tolist():\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train[f].astype(str).values) + list(test[f].astype(str).values))\n",
    "    train[f] = lbl.transform(list(train[f].astype(str).values))\n",
    "    test[f] = lbl.transform(list(test[f].astype(str).values))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\n",
    "one_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\n",
    "\n",
    "#many_null_cols = [col for col in train.columns if train[col].isnull().sum() / train.shape[0] > 0.9]\n",
    "#many_null_cols_test = [col for col in test.columns if test[col].isnull().sum() / test.shape[0] > 0.9]\n",
    "\n",
    "#big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n",
    "#big_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n",
    "\n",
    "cols_to_drop = list(set(\n",
    "    one_value_cols+ one_value_cols_test\n",
    "))\n",
    "\n",
    "# 不要な削除はダメ\n",
    "# many_null_cols + many_null_cols_test + big_top_value_cols + big_top_value_cols_test + \n",
    "\n",
    "#cols_to_drop.remove('isFraud')\n",
    "\n",
    "train.drop(cols_to_drop, axis=1, inplace=True)\n",
    "test.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多重共線性の削除 : ダメ0.98\n",
    "#threshold = 1\n",
    "#corr_matrix = train[train['isFraud'].notnull()].corr().abs()\n",
    "#upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "#to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "#print('There are %d columns to remove.' % (len(to_drop)))\n",
    "#train = train.drop(columns = to_drop)\n",
    "#test = test.drop(columns = to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コルモゴロフ-スミルノフ検定 : TODO\n",
    "# 二つの変数の違いを検定\n",
    "# 学習データとテストデータがあまりに違うものを削除\n",
    "#features_check = []\n",
    "#columns_to_check = set(list(train)).difference(base_columns)\n",
    "#for i in columns_to_check:\n",
    "#    features_check.append(ks_2samp(test[i], train[i])[1])\n",
    "\n",
    "#features_check = pd.Series(features_check, index=columns_to_check).sort_values() \n",
    "#features_discard = list(features_check[features_check==0].index)\n",
    "\n",
    "# IDなどの削除 : TODO\n",
    "# カードIDなどは入れた方がCVが上がったので入れた方がいいかも？\n",
    "rm_cols = [\n",
    "    'TransactionID_x', 'TransactionID_y',\n",
    "    'TransactionDT',\n",
    "#    'uid','uid2','uid3','uid4','uid5','uid6','uid7','uid8','bank_type',\n",
    "    'DT','DT_M','DT_W','DT_D',\n",
    "]\n",
    "#features_columns = list(train)\n",
    "#for col in rm_cols + features_discard:\n",
    "#    if col in features_columns:\n",
    "#        features_columns.remove(col)\n",
    "        \n",
    "train = train.drop(rm_cols, axis=1)# + features_discard\n",
    "test = test.drop(rm_cols, axis=1)# + features_discard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 欠損値対応"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダメだけどNNにした時に必要そう\n",
    "#train = train.fillna(-999)\n",
    "#test = test.fillna(-999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_transaction, train_identity, test_transaction, test_identity\n",
    "\n",
    "X_train = train.drop('isFraud', axis=1)\n",
    "y_train = train['isFraud'].copy()\n",
    "\n",
    "X_test = test\n",
    "\n",
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1023.30 Mb (47.1% reduction)\n",
      "Mem. usage decreased to 883.81 Mb (47.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "X_train = reduce_mem_usage(X_train)\n",
    "X_test = reduce_mem_usage(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    num_leaves = trial.suggest_loguniform('num_leaves', 100, 1000)\n",
    "    min_child_weight = trial.suggest_loguniform('min_child_weight', 0.01, 0.1)\n",
    "    feature_fraction = trial.suggest_loguniform('feature_fraction', 0.1, 1)\n",
    "    bagging_fraction = trial.suggest_loguniform('bagging_fraction', 0.1, 1)\n",
    "    min_data_in_leaf = trial.suggest_loguniform('min_data_in_leaf', 50, 150)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.01)\n",
    "    reg_alpha = trial.suggest_loguniform('reg_alpha', 0.01, 1)\n",
    "    reg_lambda = trial.suggest_loguniform('reg_lambda', 0.01, 1)\n",
    "    \n",
    "    params = {\n",
    "        'num_leaves': int(num_leaves),\n",
    "        'min_child_weight': min_child_weight, \n",
    "        'feature_fraction': feature_fraction,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'min_data_in_leaf': int(min_data_in_leaf),\n",
    "        'learning_rate': learning_rate,\n",
    "        'reg_alpha': reg_alpha,\n",
    "        'reg_lambda': reg_lambda,\n",
    "        # constant\n",
    "        'objective': 'binary',\n",
    "        'max_depth': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'bagging_seed': 11,\n",
    "        'metric': 'auc',\n",
    "        'verbosity': -1,\n",
    "        'random_state': 47,\n",
    "        'device' : 'gpu',\n",
    "        'n_jobs' : -1\n",
    "    }\n",
    "                                                \n",
    "    splits = 5\n",
    "    folds = KFold(n_splits = splits)\n",
    "                                                \n",
    "    cv_score = []\n",
    "                                                \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train.values, y_train.values)):\n",
    "        X_fit, y_fit = X_train.iloc[trn_idx], y_train.iloc[trn_idx]\n",
    "        X_val, y_val = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        trn_data = lgb.Dataset(X_fit, label=y_fit)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "        clf = lgb.train(\n",
    "            params,\n",
    "            trn_data,\n",
    "            10000,\n",
    "            valid_sets = [trn_data, val_data],\n",
    "            verbose_eval=500,\n",
    "            early_stopping_rounds=500\n",
    "        )\n",
    "                                                \n",
    "        pred = clf.predict(X_val)\n",
    "                                                \n",
    "        roc_auc = roc_auc_score(y_val, pred)\n",
    "        cv_score.append(roc_auc)\n",
    "    \n",
    "    return np.mean(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_cv_score = []\n",
    "seed_predictions = np.zeros(len(X_test))\n",
    "seeds = [3, 6, 9, 42, 47]\n",
    "\n",
    "for seed in seeds:\n",
    "    \n",
    "    lgb_params = {\n",
    "       'num_leaves': int(best_param['num_leaves']),\n",
    "       'min_child_weight': best_param['min_child_weight'],\n",
    "       'feature_fraction': best_param['feature_fraction'],\n",
    "       'bagging_fraction': best_param['bagging_fraction'],\n",
    "       'min_data_in_leaf': int(best_param['min_data_in_leaf']),\n",
    "       'learning_rate': best_param['learning_rate'],\n",
    "       'reg_alpha': best_param['reg_alpha'],\n",
    "       'reg_lambda': best_param['reg_lambda'],\n",
    "        # constant\n",
    "       'objective': 'binary',\n",
    "       'max_depth': -1,\n",
    "       'boosting_type': 'gbdt',\n",
    "       'bagging_seed': seed,\n",
    "       'metric': 'auc',\n",
    "       'verbosity': -1,\n",
    "       'random_state': seed,\n",
    "       'device' : 'gpu',\n",
    "       'n_jobs' : -1\n",
    "   }\n",
    "\n",
    "    splits = 5\n",
    "    folds = KFold(n_splits = splits)\n",
    "    oof = np.zeros(len(X_train))\n",
    "    predictions = np.zeros(len(X_test))\n",
    "\n",
    "    cv_score = []\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train.values, y_train.values)):\n",
    "        print(\"\\nFold {}\".format(fold_))\n",
    "        X_fit, y_fit = X_train.iloc[trn_idx], y_train.iloc[trn_idx]\n",
    "        X_val, y_val = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        trn_data = lgb.Dataset(X_fit, label=y_fit)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "        lgb_clf = lgb.train(\n",
    "            params,\n",
    "            trn_data,\n",
    "            10000,\n",
    "            valid_sets = [trn_data, val_data],\n",
    "            verbose_eval=500,\n",
    "            early_stopping_rounds=500\n",
    "        )\n",
    "\n",
    "        pred = lgb_clf.predict(X_val)\n",
    "        oof[val_idx] = pred\n",
    "        roc_auc = roc_auc_score(y_val, pred)\n",
    "        print( \"\\tauc = \", roc_auc)\n",
    "        cv_score.append(roc_auc)\n",
    "        predictions += lgb_clf.predict(X_test) / splits\n",
    "        \n",
    "    seed_predictions += predictions / len(seeds)\n",
    "    seed_cv_score.append(np.mean(cv_score))\n",
    "    \n",
    "print(np.mean(seed_cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = sample_submission.reset_index()\n",
    "sample_submission['isFraud'] = predictions\n",
    "sample_submission.to_csv('./submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
